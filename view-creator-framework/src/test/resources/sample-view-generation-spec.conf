view.name = myView
view.output.schema.class = org.hypertrace.core.viewcreator.test.api.TestView
view.output.schema.url = "host:port/myView"

kafka.brokerAddress = "localhost:9092"
kafka.topicName = test-view-events
kafka.partitions = 8
kafka.replicationFactor = 3

pinot.controllerHost = localhost
pinot.controllerPort = 9000
pinot.timeColumn = creation_time_millis
pinot.timeUnit = MILLISECONDS
pinot.dimensionColumns = [name, creation_time_millis, id_sha, friends, properties__KEYS, properties__VALUES]
pinot.columnsMaxLength = {id_sha: 64}
pinot.metricColumns = [time_taken_millis]
pinot.invertedIndexColumns = [friends, properties__KEYS, properties__VALUES]
pinot.tableName = myView1
pinot.loadMode = MMAP
pinot.numReplicas = 1
pinot.retentionTimeValue = 3
pinot.retentionTimeUnit = DAYS
pinot.brokerTenant = defaultBroker
pinot.serverTenant = defaultServer
pinot.segmentAssignmentStrategy = BalanceNumSegmentAssignmentStrategy

pinot.streamConfigs =
{
  streamType: kafka,
  stream.kafka.consumer.type: LowLevel,
  stream.kafka.topic.name: test-view-events,
  stream.kafka.consumer.factory.class.name: "org.apache.pinot.plugin.stream.kafka20.KafkaConsumerFactory",
  stream.kafka.decoder.class.name: "org.apache.pinot.plugin.inputformat.avro.confluent.KafkaConfluentSchemaRegistryAvroMessageDecoder",
  stream.kafka.decoder.prop.schema.registry.rest.url: "http://localhost:8081",
  stream.kafka.hlc.zk.connect.string: "localhost:2181",
  stream.kafka.zk.broker.url: "localhost:2181",
  stream.kafka.broker.list: "localhost:9092",
  realtime.segment.flush.threshold.time: 3600000,
  realtime.segment.flush.threshold.size: 500000,
  stream.kafka.consumer.prop.auto.offset.reset: largest
}
